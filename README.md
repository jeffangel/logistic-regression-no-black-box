---
title: RegresiÃ³n LogÃ­stica sin Caja Negra
emoji: ðŸ“Š
colorFrom: blue
colorTo: indigo
sdk: docker
pinned: false
---

# ðŸ“Š RegresiÃ³n LogÃ­stica sin Caja Negra: EstadÃ­stica, OptimizaciÃ³n y Algoritmos sobre un Caso Real

**[Read in English](#english-version)**

---
## En EspaÃ±ol

### ðŸ“˜ DescripciÃ³n
Proyecto para estudiar **regresiÃ³n logÃ­stica** desde la estadÃ­stica y la optimizaciÃ³n, sin depender de cajas negras. La API expone un modelo entrenado sobre el dataset numÃ©rico de crÃ©dito alemÃ¡n y devuelve predicciones de tres optimizadores distintos para comparar su comportamiento.

- ðŸ§® ImplementaciÃ³n manual de la funciÃ³n sigmoid estable
- ðŸ“‰ Ajuste de parÃ¡metros mediante tres algoritmos de optimizaciÃ³n
- ðŸ§  NormalizaciÃ³n con media y desviaciÃ³n estÃ¡ndar almacenadas en el modelo
- ðŸš€ API FastAPI lista para probar el modelo y comparar optimizadores
- ðŸ³ Contenedor Docker opcional (secundario frente a la parte matemÃ¡tica)

### ðŸ“ Blog explicativo con demo

> Para un recorrido completo paso a paso con demostraciÃ³n interactiva, [lee el artÃ­culo en mi blog](https://jeffangel.github.io/blog/regresion-logistica-glm-sin-caja-negra).


### ðŸ“ Fundamentos estadÃ­sticos y matemÃ¡ticos

- Modelo: $p(y=1 \mid \mathbf{x}) = \sigma(\mathbf{x}^\top \beta)$ con $\sigma$ estable.
- Log-verosimilitud negativa: $\ell(\beta) = - \sum_i \big[y_i \log p_i + (1-y_i) \log (1-p_i)\big]$.
- Gradiente: $\nabla \ell(\beta) = X^\top (p - y)$.
- Hessiano: $H = X^\top W X$ con $W$ diagonal y $w_i = p_i (1-p_i)$.
- NormalizaciÃ³n Z-score previa: $\tilde{x}_j = (x_j - \mu_j) / \sigma_j$ y se agrega el intercepto como primer tÃ©rmino.

### âš™ï¸ OptimizaciÃ³n implementada (tres rutas de entrenamiento)

- **Newton-Raphson**: usa $H^{-1} \nabla \ell$ para convergencia cuadrÃ¡tica en las Ãºltimas iteraciones.
- **Descenso de gradiente** (paso fijo): actualizaciÃ³n $\beta \leftarrow \beta - \alpha \nabla \ell$, estable para comparar contra Newton.
- **Descenso de gradiente con backtracking**: bÃºsqueda de paso tipo Armijo para controlar la tasa de aprendizaje y robustecer la convergencia.

Cada inferencia devuelve el resultado de los tres optimizadores para que puedas observar diferencias en $\eta$ y $p$.

> Nota conceptual:
> Los tres mÃ©todos optimizan **la misma funciÃ³n de log-verosimilitud**.
> Las diferencias observadas en las predicciones intermedias se deben
> exclusivamente al mÃ©todo de optimizaciÃ³n y a su trayectoria de convergencia,
> no a cambios en el modelo estadÃ­stico.

### ðŸ”¬ CÃ³mo se entrena

- Dataset: **Statlog (German Credit Data)** â€” 1000 observaciones, 24 variables numÃ©ricas,
  problema de clasificaciÃ³n binaria.
  Disponible en el repositorio oficial de UCI:
  https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data

- Notebooks paso a paso (derivaciones, optimizaciÃ³n y experimentos):
  - EspaÃ±ol: `notebooks/es/optimizacion_desde_cero.ipynb`
  - InglÃ©s: `notebooks/en/optimization_from_scratch.ipynb`
- El objeto entrenado contiene: medias, desviaciones estÃ¡ndar y coeficientes `beta_nr`, `beta_gd`, `beta_bt` correspondientes a Newton, GD y GD con backtracking.

### ðŸ“‚ Estructura del proyecto

```
logistic-regression-no-black-box/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ es/optimizacion_desde_cero.ipynb       # Derivaciones y entrenamiento en espaÃ±ol
â”‚   â””â”€â”€ en/optimization_from_scratch.ipynb     # Derivaciones y entrenamiento en inglÃ©s
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                                 # AplicaciÃ³n FastAPI
â”‚   â”œâ”€â”€ config.py                              # ConfiguraciÃ³n (HF_TOKEN opcional si el repo es privado)
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ health.py                          # Health check
â”‚   â”‚   â”œâ”€â”€ predict.py                         # PredicciÃ³n con tres optimizadores
â”‚   â”‚   â””â”€â”€ sample.py                          # Muestra aleatoria del dataset
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ inference_service.py               # Normaliza, agrega intercepto y aplica sigmoids
â”‚   â”‚   â””â”€â”€ sampler_service.py                 # SelecciÃ³n aleatoria de fila
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ activations.py                     # Sigmoid estable
â”‚       â””â”€â”€ startup.py                         # Descarga y carga del objeto entrenado
â”œâ”€â”€ data/german.data-numeric                   # Dataset numÃ©rico original
â”œâ”€â”€ Dockerfile                                 # Contenedor opcional
â”œâ”€â”€ requirements.txt                           # Dependencias
â”œâ”€â”€ run.sh                                     # Arranque local
â”œâ”€â”€ main.py                                    # Punto de entrada simple
â””â”€â”€ README.md
```

### ðŸ“Œ Licencia

MIT - Ãšsalo, modifÃ­calo y compÃ¡rtelo con atribuciÃ³n.

---

<a id="english-version"></a>

## In English

### ðŸ“˜ Description
Project to study **logistic regression** from a statistical and optimization-first perspective. The API serves an already trained model on the numeric German credit dataset and returns the outputs of three different optimizers so you can compare them.

- Stable sigmoid, manual normalization, no ML black boxes
- Three optimizers to contrast: Newton-Raphson, GD, GD with backtracking
- FastAPI endpoints to fetch a sample and score it with all solvers
- Docker is available but secondary to the math and demo

### ðŸ“ Walkthrough blog with demo

> For a guided walkthrough with an interactive demo, [read the article on my blog](https://jeffangel.github.io/blog/regresion-logistica-glm-sin-caja-negra).

### ðŸ“ Statistical and mathematical backbone

- Model: $p(y=1 \mid \mathbf{x}) = \sigma(\mathbf{x}^\top \beta)$.
- Negative log-likelihood: $\ell(\beta) = - \sum_i \big[y_i \log p_i + (1-y_i) \log (1-p_i)\big]$.
- Gradient: $\nabla \ell(\beta) = X^\top (p - y)$.
- Hessian: $H = X^\top W X$ with $w_i = p_i (1-p_i)$.
- Z-score normalization and intercept appended before inference.

### âš™ï¸ Optimization tracks

- **Newton-Raphson** for fast convergence using $H^{-1} \nabla \ell$.
- **Gradient descent** with fixed step for baseline behavior.
- **Gradient descent with backtracking** (Armijo-like) to adapt the learning rate per step.

Every prediction surfaces $\eta$ and $p$ from each optimizer.

> Conceptual note:
> All three methods optimize **the same log-likelihood function**.
> The differences observed in the intermediate predictions are due
> exclusively to the optimization method and its convergence trajectory,
> not to changes in the underlying statistical model.


### ðŸ”¬ Training notes

- Dataset: **Statlog (German Credit Data)** â€” 1000 observations, 24 numeric features,
  binary classification problem.
  Available from the official UCI repository:
  https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data
- Walkthrough notebooks (derivations, optimization, experiments):
  - Spanish: `notebooks/es/optimizacion_desde_cero.ipynb`
  - English: `notebooks/en/optimization_from_scratch.ipynb`
- The persisted object packs means, standard deviations and `beta_nr`, `beta_gd`, `beta_bt` coefficients.

### ðŸ“¡ API endpoints

- `/health/` â€” health check.
- `/sample/` â€” random normalized row with its class.
- `/predict/` â€” accepts `new_sample` (24-length list) and returns all three optimizers.

### ðŸ“‚ Project structure

```
logistic-regression-no-black-box/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ es/optimizacion_desde_cero.ipynb       # Spanish training notebook
â”‚   â””â”€â”€ en/optimization_from_scratch.ipynb     # English training notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                                 # FastAPI app
â”‚   â”œâ”€â”€ config.py                              # Settings (HF_TOKEN optional)
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ health.py                          # Health check
â”‚   â”‚   â”œâ”€â”€ predict.py                         # Prediction endpoint
â”‚   â”‚   â””â”€â”€ sample.py                          # Sampling endpoint
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ inference_service.py               # Normalize, intercept, sigmoid
â”‚   â”‚   â””â”€â”€ sampler_service.py                 # Random sample selection
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ activations.py                     # Stable sigmoid
â”‚       â””â”€â”€ startup.py                         # Download/load artifacts
â”œâ”€â”€ data/german.data-numeric                   # Original numeric dataset
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh
â”œâ”€â”€ main.py
â””â”€â”€ README.md
```

MIT - Feel free to use, modify and share with attribution.
